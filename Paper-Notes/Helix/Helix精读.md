本文介绍了Helix，一个在异构GPU集群上用于高吞吐量、低延迟大型语言模型（LLM）服务的分布式系统。**Helix背后的一个关键思想是将异构GPU和网络连接上的LLM推理计算公式化为一个有向、加权图的最大流问题，其中节点代表GPU实例，边通过它们的容量捕捉GPU和网络异质性。** Helix然后使用混合整数线性规划（MILP）算法发现高度优化的策略来服务LLM。这种方法使得Helix能够联合优化模型放置和请求调度，这两个在异构LLM服务中高度纠缠的任务。作者在从24到42个GPU节点的几个异构集群设置上的评估显示，与现有的最佳方法相比，Helix将服务吞吐量提高了2.7倍，将提示和解码延迟分别降低了2.8倍和1.3倍。

**有向加权图**可以定义为三元组 $G = (V, E, W)$：

- $V$：**顶点集合**（Nodes/Vertices），例如 $\{GPU_1, GPU_2, \dots \}$。
    
- $E$：**有向边集合**（Directed Edges），集合中的每条边都是一个有序对 $(u, v)$，表示从顶点 $u$ 到顶点 $v$ 的连接。
    
- $W$：**权重集合**（Weights），为 $E$ 中的每条边 $(u, v)$ 分配一个数值 $w(u, v)$。

| **图论元素**                 | **在 Helix 系统中的含义** | **说明**              |
| ------------------------ | ------------------ | ------------------- |
| **节点 (Node)**            | **GPU 实例**         | 代表参与推理计算的异构 GPU 资源。 |
| **有向边 (Directed Edge)**  | **计算或网络连接**        | 表示任务或数据流动的方向。       |
| **权重 (Weight/Capacity)** | **GPU 算力或网络带宽**    | 网络连接的传输速率（即**容量**）。 |

由于GPU架构设计的进步以及它们随时间的增量部署，现代云平台越来越多地由多种GPU类型组成。这些异构GPU实例分布在全世界的数据中心，并且总体上提供的内存容量和计算资源远大于单个GPU类型，使得LLM服务变得更加可达和可扩展。同样，也有一种趋势是使用志愿者消费者的GPU来解决GPU短缺问题。然而，与同构GPU实例相比，在地理分布式异构实例上部署LLM需要适应各种GPU设备和网络条件。

与作者最相关的工作是Petalas，它使用流水线模型并行性将LLM划分为多个阶段，并采用贪心算法以循环方式将这些阶段分配给异构GPU。这种优化吞吐量的设计对长时间运行的任务有效，但缺乏交互式LLM应用（如需要快速响应的聊天机器人）所需的响应性。
### 1. 核心痛点：它主要解决的是数据中心网络中的什么具体问题？

Helix 主要解决的是 **在异构（Heterogeneous）和跨地域（Geo-distributed）环境下，现有 LLM 服务系统因“木桶效应”导致的网络和计算资源利用率低下的问题** 。

具体来说，它针对以下三个网络与计算层面的痛点：

- **异构硬件的负载不均**：现代云平台往往混合了不同代际的 GPU（如 H100, A100, L4, T4）。现有的系统（如 Orca, vLLM）通常假设集群是同构的，倾向于均匀切分模型。如果将同样的任务分配给强 GPU 和弱 GPU，弱 GPU 会成为瓶颈，导致强 GPU 等待 。
    
- **跨地域网络的带宽瓶颈**：当 GPU 资源分布在不同物理位置（Geo-distributed）时，节点间的带宽差异巨大（例如：同一机架内 vs. 跨城市数据中心）。传统的张量并行（Tensor Parallelism）通信量极大，受限于慢速网络；而流水线并行（Pipeline Parallelism）虽然通信量较小，但如果不仅过优化，容易在慢速连接上发生拥塞。
    
- **静态路由的局限性**：现有的异构系统（如 Swarm）通常使用固定的流水线或简单的贪婪调度。这在网络波动或负载变化时，无法动态绕过拥塞链路，导致吞吐量无法达到理论上限 。
    

**一句话总结**：Helix 试图解决如何在网络带宽参差不齐、计算能力强弱不一的“杂牌军”集群中，跑出接近物理极限的 LLM 推理吞吐量。

---

### 2. 关键机制：它是如何做到的？

Helix 的核心创新是将 LLM 推理过程建模为一个 **有向加权图上的最大流问题（Max-Flow Problem）** 。
它通过两个阶段来实现这一机制：
#### 第一阶段：离线规划 (模型放置 - Model Placement)

利用混合整数线性规划（MILP）来决定将模型的哪些层放在哪个 GPU 上，以最大化整个集群的“最大流”。

**图的抽象构建机制：**

1. **节点分裂**：为了将“计算能力”转化为“流量”，Helix 将每个 GPU 节点 $c_i$ 拆分为两个点：$c_i^{in}$ 和 $c_i^{out}$。这两个点之间的边容量（Capacity）代表该 GPU 的**计算吞吐量**（token/s）。
    
2. **网络边**：不同 GPU 节点间的边，代表物理网络连接。边的容量 = **网络带宽** / Token 数据大小。这直接将网络带宽限制转化为了流量限制 。
    
3. **求解**：通过 MILP 算法寻找最优的模型切分方案，使得从 Source（输入）到 Sink（输出）的总流量最大 。
    
#### 第二阶段：在线调度 (请求路由 - Request Scheduling)

Helix 抛弃了传统的“固定流水线”，引入了 **Per-Request Pipelines（逐请求流水线）** 。

**调度流程描述：**

1. **拓扑加载**：系统维护一个基于上述 MILP 优化后的集群拓扑图，边权重对应分配的流量 。
    
2. **动态路由**：对于每一个新请求，调度器在图中寻找路径。
    
3. **加权轮询（IWRR）**：为了避免计算复杂的最大流算法在实时路径选择中的开销，Helix 在每个节点使用“交错加权轮询”（Interleaved Weighted Round-Robin）算法。节点根据预先计算的“最大流”流量比例，将请求分发给下游节点 。
    
**伪代码/逻辑流程：**

```
# 1. 离线阶段：构建与规划
Graph = InitGraph()
For node in Cluster:
    # 将计算能力建模为边的容量
    Graph.add_edge(node.in, node.out, capacity=min(ComputeSpeed, NetworkSpeed))

For link in Network:
    # 将网络带宽建模为边的容量
    Graph.add_edge(nodeA.out, nodeB.in, capacity=Bandwidth / TokenSize)

# 使用 MILP 求解器找到让 Source -> Sink 流量最大的模型放置方案
OptimalPlacement = SolveMILP(Graph) 

# 2. 在线阶段：运行时调度
While True:
    Request = GetNewRequest()
    CurrentNode = Coordinator
    
    # 动态构建流水线路径
    Pipeline = []
    While not EndOfModel:
        # IWRR: 根据最大流计算出的流量比例选择下一个节点
        # 例如：70% 流量去 FastGPU，30% 流量去 SlowGPU
        NextNode = IWRR_Select(CurrentNode.Neighbors, Weights=FlowSolution)
        
        Pipeline.append(NextNode)
        CurrentNode = NextNode
    
    # 执行推理
    ExecuteInference(Request, Pipeline)
```

---

### 3. 区别：它和传统的 RDMA 处理方式最大的不同在哪里？

这是一个非常关键的区别。Helix 和 RDMA (RoCEv2/InfiniBand) 处于网络堆栈完全不同的层级，解决的问题维度也不同。

| **特性**             | **传统 RDMA (RoCEv2 / IB)**                                                | **Helix (基于 Max-Flow 的调度)**                                       |
| ------------------ | ------------------------------------------------------------------------ | ----------------------------------------------------------------- |
| **层级 (OSI Model)** | **传输层 / 链路层 (L4/L2)**                                                    | ==**应用层 (L7) / 调度层**==                                            |
| **核心目标**           | **降低通信延迟与 CPU 开销**。通过内核旁路（Kernel Bypass）和零拷贝技术，让点对点数据传输尽可能快。             | **优化全局吞吐量 (Throughput)**。通过智能路由和负载均衡，决定数据该“走哪条路”，而不是“怎么走得快”。      |
| **对网络的感知**         | **无状态/反应式**。RDMA 网卡关注的是拥塞控制（如 DCQCN），处理的是数据包级别的丢包或延迟。                    | **有状态/全局规划**。Helix 预先知道整个网络的拓扑带宽和计算节点的处理能力，进行全局统筹规划 。             |
| **处理异构性**          | 较弱。RDMA 通常在同构的高性能网络（如全 IB 网络）中表现最好。在带宽差异巨大的链路混合中，RDMA 本身无法决定“少发点数据给慢节点”。 | **极强**。Helix 的核心就是利用 Max-Flow 算法，主动将流量（Token）分配给带宽大的链路，避开带宽小的链路 。 |
| **数据传输内容**         | 原始的内存块（Buffer）。                                                          | 具体的模型中间状态（Activation Tensors/Tokens）。                             |

**深度分析：**

- **互补关系**：Helix 并不排斥 RDMA。事实上，Helix 的底层通信（论文中使用了 ZeroMQ 15）完全可以运行在 RDMA 之上。
    
- **根本区别**：
    
    - RDMA 解决的是 **“如何让这根水管的水流得更快”**。
        
    - Helix 解决的是 **“这根水管太细了，我应该把水流分流到另一根粗水管去，或者少安排点任务给这根水管连接的节点”**。
        

在 Helix 的场景下（跨地域、异构 GPU），单纯使用 RDMA 无法解决问题，因为瓶颈往往不在于协议的 CPU 开销（RDMA 的强项），而在于**物理带宽的绝对限制**和**计算节点处理能力的失配**。Helix 通过在应用层进行流量整形（Traffic Engineering）来解决这个问题。

